{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Vision Encoder: Fine-Tuning a Vision Transformer (ViT)\n",
    "\n",
    "This notebook is the next step in your journey to understand VLMs. Here, we will build a powerful image classification model by fine-tuning a pre-trained Vision Transformer.\n",
    "\n",
    "**The Goal:** Create a custom vision encoder by adapting a large, general-purpose model to a specific dataset (CIFAR-10).\n",
    "\n",
    "**Key Steps:**\n",
    "1.  **Load CIFAR-10 Dataset**: Use `torchvision` to easily access and prepare the data.\n",
    "2.  **Define Image Transforms**: Resize and normalize the images to match the pre-trained model's requirements.\n",
    "3.  **Load a Pre-trained ViT**: Use the `timm` library to load a state-of-the-art ViT model that has already been trained on ImageNet.\n",
    "4.  **Adapt the Model**: Replace the model's final layer to fit our 10 CIFAR-10 classes.\n",
    "5.  **Fine-Tune**: Write a training loop to train the model for a few epochs on the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Installation\n",
    "First, you need the `timm` (PyTorch Image Models) library. It's the standard for working with computer vision models in PyTorch. Run this in your terminal:\n",
    "\n",
    "```bash\n",
    "pip install timm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Device Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import timm \n",
    "\n",
    "# ----- Device -----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preparation and Loading\n",
    "\n",
    "# The pre-trained ViT model expects images of size 224x224.\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Download and prepare the datasets ---\n",
    "print(\"Downloading and preparing CIFAR-10 dataset...\")\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=data_transforms)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=data_transforms)\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Dataset prepared successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Pre-trained Vision Transformer\n",
    "\n",
    "print(\"Loading pre-trained Vision Transformer (vit_base_patch16_224)...\")\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "\n",
    "\n",
    "num_in_features = model.head.in_features\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "model.head = nn.Linear(num_in_features, NUM_CLASSES)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model loaded and adapted for CIFAR-10.\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define Loss Function, Optimizer, and Hyperparameters\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 1e-4 \n",
    "EPOCHS = 3          \n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training and Evaluation Loops\n",
    "\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train() \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # 2. Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 3. Track metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        total_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Main Fine-Tuning Execution\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    test_loss, test_acc = evaluate(model, test_loader, loss_fn, device)\n",
    "    print(f\"Test Loss : {test_loss:.4f} | Test Accuracy : {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nFine-tuning finished.\")\n",
    "\n",
    "# --- Save the fine-tuned model --- \n",
    "MODEL_SAVE_PATH = '../models/vit_cifar10_finetuned.pth'\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have now successfully built a custom vision encoder. The file `vit_cifar10_finetuned.pth` contains the weights of a powerful ViT model that is now specialized in classifying CIFAR-10 images.\n",
    "\n",
    "This is a fundamental building block. The next step in your VLM journey will be to understand how to take the outputs from this vision encoder and your text encoder and combine them in a meaningful way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
