{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** This notebook documents the development and experimentation process for the VLM. The final, refactored code for training and inference can be found in the `train.py` and `inference.py` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Device Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import timm\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: New SPIN Attention and Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: The Transformer Decoder Architecture with SPIN\n",
    "\n",
    "# NEW: SPINMultiHeadAttention inspired by the paper\n",
    "class SPINMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, top_k_heads=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.top_k_heads = top_k_heads # Number of image-attentive heads to keep\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None, is_cross_attention=False):\n",
    "        B, target_len, _ = Q.size()\n",
    "        _, source_len, _ = K.size()\n",
    "        \n",
    "        Q_proj = self.W_Q(Q).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K_proj = self.W_K(K).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V_proj = self.W_V(V).view(B, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q_proj, K_proj.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, torch.finfo(scores.dtype).min)\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # --- SPIN LOGIC --- #\n",
    "        # This logic is applied only during cross-attention in the decoder\n",
    "        if is_cross_attention and self.training is False: # Apply only during inference\n",
    "            # Assuming the first part of the memory is the image patch tokens\n",
    "            # This is a simplification; a more robust implementation would track token types.\n",
    "            num_image_tokens = 197 # For ViT-Tiny (196 patch tokens + 1 CLS token)\n",
    "            \n",
    "            # Calculate the average attention paid to image tokens by each head\n",
    "            # We only care about the attention from the last query token (the one we're generating from)\n",
    "            image_attention_scores = attn[:, :, -1, :num_image_tokens].mean(dim=-1) # Shape: [Batch, num_heads]\n",
    "            \n",
    "            # Find the top-K heads with the highest image attention\n",
    "            top_k_indices = torch.topk(image_attention_scores, self.top_k_heads, dim=-1).indices\n",
    "            \n",
    "            # Create a suppression mask\n",
    "            suppression_mask = torch.zeros_like(image_attention_scores)\n",
    "            suppression_mask.scatter_(1, top_k_indices, 1.0)\n",
    "            suppression_mask = suppression_mask.view(B, self.num_heads, 1, 1)\n",
    "            \n",
    "            # Apply the mask to the value projection\n",
    "            V_proj = V_proj * suppression_mask\n",
    "        # --- END SPIN LOGIC --- #\n",
    "\n",
    "        out = torch.matmul(attn, V_proj).transpose(1, 2).contiguous().view(B, -1, self.d_model)\n",
    "        return self.W_O(out)\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = SPINMultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = SPINMultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model))\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask):\n",
    "        # Self-attention: is_cross_attention=False (default)\n",
    "        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, mask=tgt_mask)))\n",
    "        # Cross-attention: is_cross_attention=True to trigger SPIN\n",
    "        x = self.norm2(x + self.dropout(self.cross_attn(Q=x, K=memory, V=memory, is_cross_attention=True)))\n",
    "        x = self.norm3(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = SinusoidalPositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask):\n",
    "        tgt_embed = self.pos_enc(self.embedding(tgt))\n",
    "        for layer in self.layers:\n",
    "            tgt_embed = layer(tgt_embed, memory, tgt_mask)\n",
    "        return self.output_linear(tgt_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: VLM Architecture and Generation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: VLM Architecture (Revised for VQA)\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, model_name='vit_tiny_patch16_224', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.embed_dim = self.model.embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward_features(x)\n",
    "\n",
    "def sample_next_token(logits, temperature=1.0, top_k=50, top_p=0.9, repetition_penalty=1.1, generated_ids=None):\n",
    "    logits = logits / max(1e-8, temperature)\n",
    "    if repetition_penalty != 1.0 and generated_ids:\n",
    "        for token_id in set(generated_ids):\n",
    "            if token_id < len(logits):\n",
    "                logits[token_id] = logits[token_id] / repetition_penalty if logits[token_id] > 0 else logits[token_id] * repetition_penalty\n",
    "    if top_k is not None and top_k > 0:\n",
    "        topk_vals, topk_idx = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        mask = torch.full_like(logits, float('-inf'))\n",
    "        mask.scatter_(-1, topk_idx, logits.gather(-1, topk_idx))\n",
    "        logits = mask\n",
    "    if top_p is not None and 0.0 < top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        probs = F.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "        cutoff = cumulative_probs > top_p\n",
    "        cutoff[..., 0] = False\n",
    "        sorted_logits[cutoff] = float('-inf')\n",
    "        logits.scatter_(-1, sorted_indices, sorted_logits)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "class VLM(nn.Module):\n",
    "    def __init__(self, vision_encoder, text_decoder, text_dim):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.text_decoder = text_decoder\n",
    "        self.vision_projection = nn.Linear(vision_encoder.embed_dim, text_dim)\n",
    "        self.text_embedding = text_decoder.embedding\n",
    "\n",
    "    def forward(self, image, question_ids, answer_ids, tgt_mask):\n",
    "        vision_memory = self.vision_encoder(image)\n",
    "        vision_memory = self.vision_projection(vision_memory)\n",
    "        \n",
    "        question_embed = self.text_embedding(question_ids)\n",
    "        \n",
    "        # Concatenate vision and question embeddings to form the memory\n",
    "        memory = torch.cat([vision_memory, question_embed], dim=1)\n",
    "        \n",
    "        return self.text_decoder(answer_ids, memory, tgt_mask)\n",
    "\n",
    "    def generate(self, image, tokenizer, text_prompt, max_len=50,\n",
    "                 temperature=1.0, top_k=50, top_p=0.9, repetition_penalty=1.2):\n",
    "        self.eval()\n",
    "        end_token_id = tokenizer.vocab['<end>']\n",
    "        \n",
    "        # Format the prompt using the VQA template\n",
    "        full_prompt = f\"USER: <image>\\n{text_prompt}\\nASSISTANT:\"\n",
    "        prompt_ids = tokenizer.tokenize(full_prompt)\n",
    "        generated_ids = torch.tensor([prompt_ids], device=device, dtype=torch.long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vision_memory_raw = self.vision_encoder(image.unsqueeze(0))\n",
    "            image_memory = self.vision_projection(vision_memory_raw)\n",
    "            \n",
    "            # Create the memory for generation by combining image and the full prompt\n",
    "            prompt_embed = self.text_embedding(generated_ids)\n",
    "            memory = torch.cat([image_memory, prompt_embed], dim=1)\n",
    "            \n",
    "            # The decoder starts generating from the full prompt sequence\n",
    "            current_ids = generated_ids\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                sz = current_ids.size(1)\n",
    "                tgt_mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
    "                \n",
    "                output = self.text_decoder(current_ids, memory, tgt_mask)\n",
    "                next_logits = output[0, -1, :]\n",
    "                \n",
    "                # Pass only the generated part to the repetition penalty logic\n",
    "                prev_ids_for_penalty = current_ids.squeeze(0).tolist()[len(prompt_ids):]\n",
    "                \n",
    "                next_tok = sample_next_token(next_logits, temperature=temperature,\n",
    "                                             top_k=top_k, top_p=top_p,\n",
    "                                             repetition_penalty=repetition_penalty,\n",
    "                                             generated_ids=prev_ids_for_penalty)\n",
    "                if next_tok == end_token_id:\n",
    "                    break\n",
    "                current_ids = torch.cat([current_ids, torch.tensor([[next_tok]], device=device)], dim=1)\n",
    "        \n",
    "        # Decode only the generated part, excluding the prompt\n",
    "        return tokenizer.ids_to_sentence(current_ids.squeeze(0).tolist()[len(prompt_ids):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Synthetic VQA Dataset and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: NEW VQA-Style Dataset and Tokenizer\n",
    "\n",
    "class Flickr8kVQADataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_file, transform, tokenizer, max_len):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = self._load_data(captions_file)\n",
    "        self.questions = [\n",
    "            \"Describe the image in detail.\",\n",
    "            \"What is happening in this picture?\",\n",
    "            \"Provide a detailed description of the image.\",\n",
    "            \"What is in the image?\",\n",
    "            \"Can you describe this image for me?\"\n",
    "        ]\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f.readlines()[1:]:\n",
    "                parts = line.strip().split(',')\n",
    "                image_file, caption = parts[0], ','.join(parts[1:])\n",
    "                data.append({'image': image_file, 'caption': caption.lower()})\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = os.path.join(self.image_dir, item['image'])\n",
    "        image = self.transform(Image.open(image_path).convert(\"RGB\"))\n",
    "        \n",
    "        # --- VQA Formatting ---\n",
    "        question = random.choice(self.questions)\n",
    "        answer = item['caption']\n",
    "        \n",
    "        prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
    "        \n",
    "        prompt_tokens = self.tokenizer.tokenize(prompt)\n",
    "        answer_tokens = self.tokenizer.tokenize(answer)\n",
    "        \n",
    "        # Decoder input is the answer sequence, starting with <start>\n",
    "        answer_input_ids = [self.tokenizer.vocab['<start>']] + answer_tokens\n",
    "        # Target is the answer sequence, ending with <end>\n",
    "        answer_target_ids = answer_tokens + [self.tokenizer.vocab['<end>']]\n",
    "\n",
    "        # Pad sequences\n",
    "        question_ids = (prompt_tokens + [self.tokenizer.vocab['<pad>']] * self.max_len)[:self.max_len]\n",
    "        answer_input_ids = (answer_input_ids + [self.tokenizer.vocab['<pad>']] * self.max_len)[:self.max_len]\n",
    "        answer_target_ids = (answer_target_ids + [self.tokenizer.vocab['<pad>']] * self.max_len)[:self.max_len]\n",
    "        \n",
    "        return image, torch.tensor(question_ids), torch.tensor(answer_input_ids), torch.tensor(answer_target_ids)\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_path):\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "        if '<image>' not in self.vocab: self.vocab['<image>'] = len(self.vocab)\n",
    "        if '<start>' not in self.vocab: self.vocab['<start>'] = len(self.vocab)\n",
    "        if '<end>' not in self.vocab: self.vocab['<end>'] = len(self.vocab)\n",
    "        if '<pad>' not in self.vocab: self.vocab['<pad>'] = 0\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Handle special tokens first\n",
    "        text = text.replace('<image>', ' <image> ')\n",
    "        tokens = re.findall(r'<image>|\\b\\w+\\b|\\S', text.lower())\n",
    "        return [self.vocab.get(t, self.vocab.get('<unk>', 1)) for t in tokens]\n",
    "\n",
    "    def ids_to_sentence(self, ids):\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            word = self.inv_vocab.get(i, '<unk>')\n",
    "            if word in ['<start>', '<pad>', '<image>']: continue\n",
    "            if word == '<end>': break\n",
    "            words.append(word)\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Main Setup and Two-Stage Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Main Setup for Two-Stage Training\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "D_FF = 1024\n",
    "NUM_DECODER_LAYERS = 4\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 60 # Increased slightly for prompt template\n",
    "\n",
    "# --- Two-Stage Training Hyperparameters ---\n",
    "WARMUP_EPOCHS = 10 \n",
    "FINETUNE_EPOCHS = 5\n",
    "WARMUP_LR = 1e-4\n",
    "FINETUNE_LR = 1e-6\n",
    "\n",
    "# --- Robust Training Hyperparameters ---\n",
    "ACCUMULATION_STEPS = 4\n",
    "PATIENCE = 3\n",
    "\n",
    "# --- Paths ---\n",
    "IMAGE_DIR = './flickr8k/Images'\n",
    "CAPTIONS_FILE = './flickr8k/captions.txt'\n",
    "VOCAB_PATH = 'finetuned_vocab.json'\n",
    "BEST_MODEL_PATH = 'vlm_spin_best_model.pth'\n",
    "\n",
    "# --- Tokenizer, Transforms ---\n",
    "tokenizer = SimpleTokenizer(VOCAB_PATH)\n",
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Dataset and DataLoader with VQA-style data ---\n",
    "full_dataset = Flickr8kVQADataset(IMAGE_DIR, CAPTIONS_FILE, image_transforms, tokenizer, MAX_LEN)\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, questions, answer_inputs, answer_targets = zip(*batch)\n",
    "    return torch.stack(images, 0), torch.stack(questions, 0), torch.stack(answer_inputs, 0), torch.stack(answer_targets, 0)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "print(f\"Dataset split: {len(train_dataset)} training samples, {len(val_dataset)} validation samples.\")\n",
    "\n",
    "# --- Model Instantiation ---\n",
    "vision_encoder = VisionEncoder(model_name='vit_tiny_patch16_224')\n",
    "text_decoder = TransformerDecoder(VOCAB_SIZE, D_MODEL, NUM_HEADS, D_FF, NUM_DECODER_LAYERS)\n",
    "vlm = VLM(vision_encoder, text_decoder, text_dim=D_MODEL).to(device)\n",
    "\n",
    "# --- Loss Function ---\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.vocab['<pad>'], label_smoothing=0.1)\n",
    "\n",
    "# --- Optimizers and Schedulers ---\n",
    "warmup_params = list(vlm.text_decoder.parameters()) + list(vlm.vision_projection.parameters())\n",
    "optimizer_warmup = torch.optim.AdamW(warmup_params, lr=WARMUP_LR)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_warmup, 'min', patience=1, factor=0.5)\n",
    "\n",
    "optimizer_finetune = torch.optim.AdamW(vlm.parameters(), lr=FINETUNE_LR)\n",
    "scheduler_finetune = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_finetune, 'min', patience=1, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Two-Stage Training Loop\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def create_causal_mask(sz, device):\n",
    "    return torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
    "\n",
    "scaler = GradScaler()\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"\\n--- STARTING STAGE 1: DECODER WARM-UP ---\")\n",
    "\n",
    "for epoch in range(WARMUP_EPOCHS):\n",
    "    print(f\"\\n--- Warm-up Epoch {epoch + 1}/{WARMUP_EPOCHS} ---\")\n",
    "    vlm.vision_encoder.eval()\n",
    "    vlm.text_decoder.train()\n",
    "    vlm.vision_projection.train()\n",
    "    for param in vlm.vision_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Training Warm-up {epoch+1}\")\n",
    "    optimizer_warmup.zero_grad()\n",
    "\n",
    "    for i, (images, q_ids, a_in_ids, a_out_ids) in enumerate(pbar):\n",
    "        images, q_ids, a_in_ids, a_out_ids = images.to(device), q_ids.to(device), a_in_ids.to(device), a_out_ids.to(device)\n",
    "        tgt_mask = create_causal_mask(a_in_ids.size(1), device)\n",
    "\n",
    "        with autocast():\n",
    "            logits = vlm(images, q_ids, a_in_ids, tgt_mask)\n",
    "            loss = loss_fn(logits.view(-1, VOCAB_SIZE), a_out_ids.view(-1))\n",
    "            loss = loss / ACCUMULATION_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        total_train_loss += loss.item() * ACCUMULATION_STEPS\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer_warmup)\n",
    "            torch.nn.utils.clip_grad_norm_(warmup_params, max_norm=1.0)\n",
    "            scaler.step(optimizer_warmup)\n",
    "            scaler.update()\n",
    "            optimizer_warmup.zero_grad()\n",
    "        pbar.set_postfix({\"loss\": f\"{total_train_loss / (i + 1):.4f}\"})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Average Warm-up Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    vlm.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, q_ids, a_in_ids, a_out_ids in tqdm(val_dataloader, desc=\"Validating Warm-up\"):\n",
    "            images, q_ids, a_in_ids, a_out_ids = images.to(device), q_ids.to(device), a_in_ids.to(device), a_out_ids.to(device)\n",
    "            tgt_mask = create_causal_mask(a_in_ids.size(1), device)\n",
    "            logits = vlm(images, q_ids, a_in_ids, tgt_mask)\n",
    "            loss = loss_fn(logits.view(-1, VOCAB_SIZE), a_out_ids.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Average Warm-up Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    old_lr = optimizer_warmup.param_groups[0]['lr']\n",
    "    scheduler_warmup.step(avg_val_loss)\n",
    "    new_lr = optimizer_warmup.param_groups[0]['lr']\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"Warm-up learning rate reduced to {new_lr}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(vlm.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"New best model saved to {BEST_MODEL_PATH}\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(\"Early stopping triggered for warm-up.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- LOADING BEST MODEL FOR FINE-TUNING ---\")\n",
    "vlm.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "print(\"\\n--- STARTING STAGE 2: FULL FINE-TUNING ---\")\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(FINETUNE_EPOCHS):\n",
    "    print(f\"\\n--- Fine-tuning Epoch {epoch + 1}/{FINETUNE_EPOCHS} ---\")\n",
    "    vlm.train()\n",
    "    for param in vlm.vision_encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    total_train_loss = 0\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Training Fine-tune {epoch+1}\")\n",
    "    optimizer_finetune.zero_grad()\n",
    "\n",
    "    for i, (images, q_ids, a_in_ids, a_out_ids) in enumerate(pbar):\n",
    "        images, q_ids, a_in_ids, a_out_ids = images.to(device), q_ids.to(device), a_in_ids.to(device), a_out_ids.to(device)\n",
    "        tgt_mask = create_causal_mask(a_in_ids.size(1), device)\n",
    "\n",
    "        with autocast():\n",
    "            logits = vlm(images, q_ids, a_in_ids, tgt_mask)\n",
    "            loss = loss_fn(logits.view(-1, VOCAB_SIZE), a_out_ids.view(-1))\n",
    "            loss = loss / ACCUMULATION_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        total_train_loss += loss.item() * ACCUMULATION_STEPS\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer_finetune)\n",
    "            torch.nn.utils.clip_grad_norm_(vlm.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer_finetune)\n",
    "            scaler.update()\n",
    "            optimizer_finetune.zero_grad()\n",
    "        pbar.set_postfix({\"loss\": f\"{total_train_loss / (i + 1):.4f}\"})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Average Fine-tuning Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    vlm.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, q_ids, a_in_ids, a_out_ids in tqdm(val_dataloader, desc=\"Validating Fine-tune\"):\n",
    "            images, q_ids, a_in_ids, a_out_ids = images.to(device), q_ids.to(device), a_in_ids.to(device), a_out_ids.to(device)\n",
    "            tgt_mask = create_causal_mask(a_in_ids.size(1), device)\n",
    "            logits = vlm(images, q_ids, a_in_ids, tgt_mask)\n",
    "            loss = loss_fn(logits.view(-1, VOCAB_SIZE), a_out_ids.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Average Fine-tuning Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    old_lr = optimizer_finetune.param_groups[0]['lr']\n",
    "    scheduler_finetune.step(avg_val_loss)\n",
    "    new_lr = optimizer_finetune.param_groups[0]['lr']\n",
    "    if new_lr < old_lr:\n",
    "        print(f\"Fine-tuning learning rate reduced to {new_lr}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(vlm.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"New best model saved to {BEST_MODEL_PATH}\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(\"Early stopping triggered for fine-tuning.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nFull training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Final Corrected Inference Cell\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import traceback\n",
    "import inspect\n",
    "\n",
    "# Load the best model weights for inference\n",
    "if os.path.exists(BEST_MODEL_PATH):\n",
    "    print(f\"Loading best model from {BEST_MODEL_PATH} for inference.\")\n",
    "    vlm.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "else:\n",
    "    print(\"No trained model found. Using initial model for inference.\")\n",
    "\n",
    "vlm.eval()\n",
    "sample_image = None\n",
    "raw_image = None\n",
    "\n",
    "if len(val_dataset) > 0:\n",
    "    try:\n",
    "        random_idx = random.randint(0, len(val_dataset) - 1)\n",
    "        original_idx = val_dataset.indices[random_idx]\n",
    "        image_path = os.path.join(IMAGE_DIR, full_dataset.data[original_idx]['image'])\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "            sample_image = image_transforms(raw_image).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the image: {e}\")\n",
    "\n",
    "if raw_image is not None:\n",
    "    print(\"--- Displaying Random Image for Inference ---\")\n",
    "    plt.imshow(raw_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Could not load a sample image to display.\")\n",
    "\n",
    "# --- 1. Image + Text (VQA) ---\n",
    "print(\"\\n--- Testing Multimodal VQA ---\")\n",
    "if sample_image is not None:\n",
    "    gen_signature = inspect.signature(vlm.generate)\n",
    "    available_params = list(gen_signature.parameters.keys())\n",
    "    \n",
    "    generation_kwargs = {\n",
    "        'top_p': 0.9,\n",
    "        'repetition_penalty': 1.2\n",
    "    }\n",
    "    \n",
    "    # --- THE FIX: Add 'max_len' to the list of possible names ---\n",
    "    length_param_found = None\n",
    "    possible_names = ['max_length', 'max_tokens', 'max_new_tokens', 'max_len'] # <-- ADDED 'max_len'\n",
    "    \n",
    "    for name in possible_names:\n",
    "        if name in available_params:\n",
    "            generation_kwargs[name] = 128\n",
    "            length_param_found = name\n",
    "            break\n",
    "            \n",
    "    if length_param_found:\n",
    "        print(f\"✅ Found valid length parameter: '{length_param_found}'. Using it for generation.\")\n",
    "    else:\n",
    "        print(f\"⚠️ WARNING: Could not find a known length parameter in {available_params}.\")\n",
    "        \n",
    "    prompts = [\"what is in the image?\", \"Describe the scene.\"]\n",
    "    \n",
    "    for question in prompts:\n",
    "        print(\"-----------------------------------------\")\n",
    "        try:\n",
    "            vqa_answer = vlm.generate(\n",
    "                sample_image, \n",
    "                tokenizer, \n",
    "                text_prompt=question, \n",
    "                **generation_kwargs\n",
    "            )\n",
    "            \n",
    "            print(f\"Image + Prompt: '{question}'\")\n",
    "            print(f\"--> Generated Output: {vqa_answer}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! AN ERROR OCCURRED DURING GENERATION FOR PROMPT: '{question}' !!!\")\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    print(\"Sample image not found. Skipping multimodal test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code searches the tokenizer's vocabulary for the EOS token ID\n",
    "\n",
    "try:\n",
    "    # Check if the vocab attribute exists and is a dictionary\n",
    "    if hasattr(tokenizer, 'vocab') and isinstance(tokenizer.vocab, dict):\n",
    "        print(\"Searching the tokenizer.vocab dictionary...\")\n",
    "        \n",
    "        found_eos = False\n",
    "        # Iterate through all token-ID pairs in the vocabulary\n",
    "        for token_str, token_id in tokenizer.vocab.items():\n",
    "            # Look for common substrings in the special token's name\n",
    "            if '<eos>' in token_str or '<|endoftext|>' in token_str or '</s>' in token_str:\n",
    "                print(f\"✅ Found it! The EOS token is '{token_str}' with ID: {token_id}\")\n",
    "                found_eos = True\n",
    "                break # Stop after finding the first one\n",
    "                \n",
    "        if not found_eos:\n",
    "            print(\"Could not find a specific EOS token, but here are other special tokens:\")\n",
    "            # As a fallback, print any special token (usually enclosed in <>)\n",
    "            for token_str, token_id in tokenizer.vocab.items():\n",
    "                 if token_str.startswith('<') and token_str.endswith('>'):\n",
    "                        print(f\"--> Found special token: '{token_str}' with ID: {token_id}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: 'tokenizer.vocab' is not a dictionary or does not exist.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
